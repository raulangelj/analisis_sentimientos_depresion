{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo para anlisis de sentmientos para luego precedir si tiene sintomas de depresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Nov 03 12:05:12 +0000 2017</td>\n",
       "      <td>926419989107798016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Nov 26 18:40:28 +0000 2017</td>\n",
       "      <td>934854385577943041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 25 15:40:45 +0000 2017</td>\n",
       "      <td>945318406441635840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Nov 06 14:18:35 +0000 2017</td>\n",
       "      <td>927540721296568320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 01 23:00:57 +0000 2018</td>\n",
       "      <td>947965901332197376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  is_reply  reply_count  retweet_count   \n",
       "0           neutral     False            0              0  \\\n",
       "1           neutral      True            0              0   \n",
       "2          negative     False            0              0   \n",
       "3          negative     False            0              0   \n",
       "4          positive      True            0              0   \n",
       "\n",
       "                                                text tweet_coord   \n",
       "0  Trabajar en #Ryanair como #TMA: https://t.co/r...         NaN  \\\n",
       "1  @Iberia @FIONAFERRER Cuando gusten en Cancún s...         NaN   \n",
       "2  Sabiais que @Iberia te trata muy bien en santi...         NaN   \n",
       "3  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...         NaN   \n",
       "4  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...         NaN   \n",
       "\n",
       "                    tweet_created            tweet_id tweet_location   \n",
       "0  Fri Nov 03 12:05:12 +0000 2017  926419989107798016            NaN  \\\n",
       "1  Sun Nov 26 18:40:28 +0000 2017  934854385577943041            NaN   \n",
       "2  Mon Dec 25 15:40:45 +0000 2017  945318406441635840            NaN   \n",
       "3  Mon Nov 06 14:18:35 +0000 2017  927540721296568320            NaN   \n",
       "4  Mon Jan 01 23:00:57 +0000 2018  947965901332197376            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                      Madrid  \n",
       "1                 Mexico City  \n",
       "2                      Madrid  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                Buenos Aires  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data from tweets_public.csv and create a dataframe\n",
    "df = pd.read_csv('data/tweets_public.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with only the text and airline_sentiment columns and tweet id with the name df_sentiment\n",
    "df_sentiment = df[['text', 'airline_sentiment', 'tweet_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Temp\\ipykernel_7712\\3390815067.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sentiment['text'] = df_sentiment['text'].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en #ryanair como #tma: https://t.co/r...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancún s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair.\\n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @iberia @mitsubishi_es @bfg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0  trabajar en #ryanair como #tma: https://t.co/r...           neutral  \\\n",
       "1  @iberia @fionaferrer cuando gusten en cancún s...           neutral   \n",
       "2  sabiais que @iberia te trata muy bien en santi...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair.\\n...          negative   \n",
       "4  @cris_tortu @dakar @iberia @mitsubishi_es @bfg...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the text letters to lowercase\n",
    "df_sentiment['text'] = df_sentiment['text'].str.lower()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en #ryanair como #tma:</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancún s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair.\\n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @iberia @mitsubishi_es @bfg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                   trabajar en #ryanair como #tma:            neutral  \\\n",
       "1  @iberia @fionaferrer cuando gusten en cancún s...           neutral   \n",
       "2  sabiais que @iberia te trata muy bien en santi...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair.\\n...          negative   \n",
       "4  @cris_tortu @dakar @iberia @mitsubishi_es @bfg...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Referencia: https://stackoverflow.com/questions/6718633/python-regular-expression-again-match-url\n",
    "# remove the urls from the text but keep all the text after the url\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: re.split('http[s]*\\S+', str(x))[0])\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en ryanair como tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer cuando gusten en cancún se ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que iberia te trata muy bien en santia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair\\nb...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                      trabajar en ryanair como tma            neutral  \\\n",
       "1  iberia fionaferrer cuando gusten en cancún se ...           neutral   \n",
       "2  sabiais que iberia te trata muy bien en santia...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair\\nb...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the punctuation from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en ryanair como tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer cuando gusten en cancún se ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que iberia te trata muy bien en santia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair bu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                      trabajar en ryanair como tma            neutral  \\\n",
       "1  iberia fionaferrer cuando gusten en cancún se ...           neutral   \n",
       "2  sabiais que iberia te trata muy bien en santia...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair bu...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the \\n to a space\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the stopwrods from the text\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the emojis from the text\n",
    "\n",
    "# regular expression pattern to remove emojis from text\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # faces\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # simbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# use lambda function to remove the emojis from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the numbers from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Temp\\ipykernel_7712\\4261467530.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sentiment['label'] = df_sentiment['airline_sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7862</th>\n",
       "      <td>@Iberia @iberiaexpress especialistas en dejart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7863</th>\n",
       "      <td>Con @Iberia, mi destino a un solo click. ¡Dese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>@Iberia Muy bien. Muchas gracias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>Es que volar con Ryanair es peor que irte a ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>Iberia inaugura un nuevo espacio Premium para ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7867 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Trabajar en #Ryanair como #TMA: https://t.co/r...      1\n",
       "1     @Iberia @FIONAFERRER Cuando gusten en Cancún s...      1\n",
       "2     Sabiais que @Iberia te trata muy bien en santi...      0\n",
       "3     NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...      0\n",
       "4     @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...      1\n",
       "...                                                 ...    ...\n",
       "7862  @Iberia @iberiaexpress especialistas en dejart...      0\n",
       "7863  Con @Iberia, mi destino a un solo click. ¡Dese...      1\n",
       "7864                   @Iberia Muy bien. Muchas gracias      1\n",
       "7865  Es que volar con Ryanair es peor que irte a ch...      0\n",
       "7866  Iberia inaugura un nuevo espacio Premium para ...      0\n",
       "\n",
       "[7867 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = df[['text', 'airline_sentiment']]\n",
    "# df_sentiment['label'] = df_sentiment['airline_sentiment'].apply(lambda x: '1 start' if x == 'negative' else '5 stars')\n",
    "df_sentiment['label'] = df_sentiment['airline_sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df_sentiment = df_sentiment[['text', 'label']]\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df_sentiment.to_csv('data/clean_tweets_sentiment.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preload1 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# model_preload = \"distilbert-base-uncased\"\n",
    "# model_preload = \"xlm-roberta-base\"\n",
    "# model_preload = \"distilbert-base-multilingual-cased\"\n",
    "# model_preload = \"bert-base-multilingual-cased\"\n",
    "model_preload =  'dccuchile/bert-base-spanish-wwm-cased'\n",
    "# model_preload = \"RoBERTa-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\"Amo el mundo en el que vivo\", \"Odio a todas las personas que conozco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9905071258544922},\n",
       " {'label': 'NEGATIVE', 'score': 0.9892126321792603}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9994358420372009}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline('no estoy normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7264281511306763},\n",
       " {'label': '5 stars', 'score': 0.6389875411987305}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model = pipeline(\"sentiment-analysis\", model=model_preload1)\n",
    "specific_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '3 stars', 'score': 0.3511374890804291}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model('estoy normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.5364329814910889}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model('estoy triste')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/raula/.cache/huggingface/datasets/csv/default-166099f8577deca8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7239aed7bba4b6dbb42b1eb3ccae184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b47e9906dfa401bb94f7a388e9a401c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9158e7c8a8c74c658e63186372af9838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/raula/.cache/huggingface/datasets/csv/default-166099f8577deca8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/raula/.cache/huggingface/datasets/csv/default-166099f8577deca8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "local_train = load_dataset(\"csv\", data_files=\"data/clean_tweets_sentiment.csv\", split=\"train[:70%]\")\n",
    "local_test = load_dataset(\"csv\", data_files=\"data/clean_tweets_sentiment.csv\", split=\"train[70%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5507, 2)\n",
      "(2360, 2)\n",
      "{'text': 'Trabajar en #Ryanair como #TMA: https://t.co/ruUArBe1tO #empleo', 'label': 1}\n",
      "{'text': '@radioledonline @Iberia @lauraluzo Sr. Gallego, por qué no pagáis las indemnizaciones por cancelaciones? Aviación C… https://t.co/pC3cvISmjH', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(local_train.shape)\n",
    "print(local_test.shape)\n",
    "print(local_train[0])\n",
    "print(local_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = local_train.shuffle(seed=42).select(\n",
    "    list(list(range(300)))\n",
    ")\n",
    "small_test_dataset = local_test.shuffle(seed=42).select(list(list(range(100))))\n",
    "# # split df_sentiment in 70% train and 30% test\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, test = train_test_split(df_sentiment, test_size=0.3, random_state=42)\n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_preload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6154e85e2dc642ebba5d67017ab1d013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41b1a8abc94fad88f4bbe7b81d7800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# user the function preprocess_function to tokenize the train and test data\n",
    "# tokenized_train = train['text'].apply(lambda x: tokenizer(x, truncation=True))\n",
    "# tokenized_test = test['text'].apply(lambda x: tokenizer(x, truncation=True))\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_preload, num_labels=2, id2label={0: \"negative\", 1: \"positive\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1739dbdec954418580bc3cd6faed351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raula\\Documents\\RAUL_ANGEL\\UVG_COMPU\\NOVENO_SEMESTRE\\DISENO-E-INOVACION\\analisis_sentimientos_depresion\\raulangelj/huggingface_sentiment_analysis is already a clone of https://huggingface.co/raulangelj/huggingface_sentiment_analysis. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"raulangelj/huggingface_sentiment_analysis\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=25,\n",
    "   per_device_eval_batch_size=25,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=True,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cd084c42ae4863b152047027386143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 37.0528, 'train_samples_per_second': 16.193, 'train_steps_per_second': 0.648, 'train_loss': 0.648597240447998, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.648597240447998, metrics={'train_runtime': 37.0528, 'train_samples_per_second': 16.193, 'train_steps_per_second': 0.648, 'train_loss': 0.648597240447998, 'epoch': 2.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60efbc8c85049f59385208b6ce3d726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6742175221443176,\n",
       " 'eval_accuracy': 0.56,\n",
       " 'eval_f1': 0.6451612903225805,\n",
       " 'eval_runtime': 1.9528,\n",
       " 'eval_samples_per_second': 51.209,\n",
       " 'eval_steps_per_second': 2.048,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "To https://huggingface.co/raulangelj/huggingface_sentiment_analysis\n",
      "   f00fff2..166666a  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amo el mundo en el que vivo', 'Odio a todas las personas que conozco']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.5367423892021179},\n",
       " {'label': 'negative', 'score': 0.5140738487243652}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "sentiment_model = pipeline(model=repo_name)\n",
    "sentiment_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.5318524837493896}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model('no me siento bien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.5589547753334045}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model('hoy es el mejor dia de mi vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.5341178178787231}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model('ya no puedo con mi vida, no quiero que me hablen')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion a utilizar en proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from transformers import pipeline\n",
    "\n",
    "def has_first_person_pronouns(text):\n",
    "    first_person_pronouns = [\"yo\", \"me\", \"mi\", \"conmigo\"]\n",
    "    \n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        normalize_word = remove_accent(word.lower())\n",
    "        if normalize_word in first_person_pronouns:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def remove_accent(word):\n",
    "    return ''.join((c for c in unicodedata.normalize('NFD', word) if unicodedata.category(c) != 'Mn'))\n",
    "\n",
    "# ? should we use this?\n",
    "def has_depression_words(text):\n",
    "    depression_words = [\n",
    "        \"depresion\", \"tristeza\", \"desesperanza\", \"desmotivacion\", \"soledad\", \"ansiedad\",\n",
    "        \"desgano\", \"desanimo\", \"desaliento\", \"abatimiento\", \"melancolia\", \"desolacion\",\n",
    "        \"apatia\", \"angustia\", \"pesimismo\", \"desesperacion\", \"desamparo\", \"desconsuelo\",\n",
    "        \"agotamiento\", \"cansancio\", \"desinteres\", \"insomnio\", \"culpa\", \"llanto\", \"suicidio\",\n",
    "        \"autolesion\", \"desorden\", \"trastorno\", \"descontrol\", \"vacio\", \"apetito\", \"fatiga\",\n",
    "        \"inutilidad\", \"retroceso\", \"preocupacion\", \"retraimiento\", \"negatividad\", \"desvalorizacion\",\n",
    "        \"desesperado\", \"desesperanza\", \"perdida\", \"esperanza\", \"autoestima\", \"autoestima\",\n",
    "        \"irritabilidad\", \"decision\", \"concentracion\", \"motivacion\", \"indefension\", \"vida\",\n",
    "        \"conexion\", \"pena\", \"abandono\", \"inseguridad\", \"desapego\", \"agonia\", \"miedo\", \"rumiacion\",\n",
    "        \"tratamiento\", \"terapia\", \"psicologo\", \"psiquiatra\", \"medicacion\", \"cura\", \"superacion\",\n",
    "        \"autoayuda\", \"apoyo\", \"rehabilitacion\"\n",
    "    ]\n",
    "    \n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        normalize_word = remove_accent(word.lower())\n",
    "        if normalize_word in depression_words:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def has_possible_depression_sintom(text):\n",
    "    # To know if has depression sintoms, we will check 3 things:\n",
    "    # 1. If the text has first person pronouns\n",
    "    # 2. If the text has negative sentiment\n",
    "    # 3. If the text has words related to depression\n",
    "    # If the text has the 3 things, we will say that the text has depression sintoms\n",
    "    # If the text has 2 of the 3 things, we will say that the text has possible depression sintoms\n",
    "    # If the text has 1 of the 3 things, we will say that the text has no depression sintoms\n",
    "    sentiment_model = pipeline(model=\"raulangelj/huggingface_sentiment_analysis\")\n",
    "    is_negative_sentiment = sentiment_model(text)[0]['label'] == 'negative'\n",
    "    # sentiment_model(data)\n",
    "    is_depression_words = has_depression_words(text)\n",
    "    is_first_person_pronouns = has_first_person_pronouns(text)\n",
    "    # if all the conditions are true, then the text has depression sintoms\n",
    "    if is_negative_sentiment and is_depression_words and is_first_person_pronouns:\n",
    "        # return 'depression'\n",
    "        return True\n",
    "    # if 2 of the 3 conditions are true, then the text has possible depression sintoms\n",
    "    # if (is_negative_sentiment and is_depression_words) or (is_negative_sentiment and is_first_person_pronouns) or (is_depression_words and is_first_person_pronouns):\n",
    "    #     # return 'possible_depression'\n",
    "    #     return True\n",
    "    # if 1 of the 3 conditions are true, then the text has no depression sintoms\n",
    "    if is_negative_sentiment or is_depression_words or is_first_person_pronouns:\n",
    "        # return 'no_depression'\n",
    "        return False\n",
    "    # if none of the conditions are true, then the text has no depression sintoms\n",
    "    # return 'no_depression'\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "example_text_depression = \"Últimamente, me cuesta encontrar motivación para levantarme de la cama por las mañanas. Siento una tristeza constante que no puedo sacudir, como si estuviera atrapado en una nube gris. Incluso las actividades que solían traerme alegría ahora me resultan abrumadoras y sin sentido. Me siento agotado todo el tiempo, tanto física como emocionalmente. La soledad se ha convertido en mi compañera constante, y me resulta difícil conectar con los demás. No puedo evitar sentir que he perdido el control de mi vida y que no hay esperanza de que las cosas mejoren. Estos pensamientos negativos y la sensación de vacío me invaden, y a veces me encuentro llorando sin razón aparente. Me preocupa que estos sentimientos persistan y no sé qué hacer para superar esta desesperanza abrumadora.\"\n",
    "example_text_no_depression = \"El sol brilla radiante en el cielo azul mientras disfruto de un paseo por el parque. Río y converso con mis amigos, sintiéndome lleno de energía y entusiasmo. Me siento agradecido por las bendiciones en mi vida y tengo metas emocionantes que persigo con determinación. Cada día es una oportunidad para crecer, aprender y disfrutar de las pequeñas cosas que hacen que la vida sea maravillosa. Me rodeo de personas positivas que me apoyan y me animan en mis sueños. La vida es un viaje emocionante y estoy emocionado de ver lo que el futuro me depara.\"\n",
    "example_text_no_depression_sad = \"Atravesé un momento difícil en mi vida recientemente. Experimenté una pérdida personal significativa que me dejó con el corazón apesadumbrado. Durante un tiempo, sentí una profunda tristeza y una sensación de vacío en mi interior. Sin embargo, me permití sentir y procesar estas emociones, encontrando consuelo en el amor y el apoyo de mis seres queridos. Poco a poco, comencé a encontrar la paz dentro de mí y a enfocarme en las cosas positivas que todavía tengo en mi vida. Aprendí a apreciar más cada momento y a encontrar la fortaleza para seguir adelante. Aunque todavía tengo momentos de tristeza, también encuentro alegría y esperanza en las pequeñas cosas de la vida. La adversidad me ha enseñado a ser más resiliente y a valorar cada experiencia, tanto positiva como negativa.\"\n",
    "print(has_possible_depression_sintom(example_text_depression))\n",
    "print(has_possible_depression_sintom(example_text_no_depression))\n",
    "print(has_possible_depression_sintom(example_text_no_depression_sad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
