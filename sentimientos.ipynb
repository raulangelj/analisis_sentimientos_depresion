{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo para anlisis de sentmientos para luego precedir si tiene sintomas de depresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Nov 03 12:05:12 +0000 2017</td>\n",
       "      <td>926419989107798016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Nov 26 18:40:28 +0000 2017</td>\n",
       "      <td>934854385577943041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 25 15:40:45 +0000 2017</td>\n",
       "      <td>945318406441635840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Nov 06 14:18:35 +0000 2017</td>\n",
       "      <td>927540721296568320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 01 23:00:57 +0000 2018</td>\n",
       "      <td>947965901332197376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  is_reply  reply_count  retweet_count   \n",
       "0           neutral     False            0              0  \\\n",
       "1           neutral      True            0              0   \n",
       "2          negative     False            0              0   \n",
       "3          negative     False            0              0   \n",
       "4          positive      True            0              0   \n",
       "\n",
       "                                                text tweet_coord   \n",
       "0  Trabajar en #Ryanair como #TMA: https://t.co/r...         NaN  \\\n",
       "1  @Iberia @FIONAFERRER Cuando gusten en Cancún s...         NaN   \n",
       "2  Sabiais que @Iberia te trata muy bien en santi...         NaN   \n",
       "3  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...         NaN   \n",
       "4  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...         NaN   \n",
       "\n",
       "                    tweet_created            tweet_id tweet_location   \n",
       "0  Fri Nov 03 12:05:12 +0000 2017  926419989107798016            NaN  \\\n",
       "1  Sun Nov 26 18:40:28 +0000 2017  934854385577943041            NaN   \n",
       "2  Mon Dec 25 15:40:45 +0000 2017  945318406441635840            NaN   \n",
       "3  Mon Nov 06 14:18:35 +0000 2017  927540721296568320            NaN   \n",
       "4  Mon Jan 01 23:00:57 +0000 2018  947965901332197376            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                      Madrid  \n",
       "1                 Mexico City  \n",
       "2                      Madrid  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                Buenos Aires  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data from tweets_public.csv and create a dataframe\n",
    "df = pd.read_csv('data/tweets_public.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with only the text and airline_sentiment columns and tweet id with the name df_sentiment\n",
    "df_sentiment = df[['text', 'airline_sentiment', 'tweet_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Temp\\ipykernel_12380\\3390815067.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sentiment['text'] = df_sentiment['text'].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en #ryanair como #tma: https://t.co/r...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancún s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair.\\n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @iberia @mitsubishi_es @bfg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0  trabajar en #ryanair como #tma: https://t.co/r...           neutral  \\\n",
       "1  @iberia @fionaferrer cuando gusten en cancún s...           neutral   \n",
       "2  sabiais que @iberia te trata muy bien en santi...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair.\\n...          negative   \n",
       "4  @cris_tortu @dakar @iberia @mitsubishi_es @bfg...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the text letters to lowercase\n",
    "df_sentiment['text'] = df_sentiment['text'].str.lower()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en #ryanair como #tma:</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancún s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair.\\n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @iberia @mitsubishi_es @bfg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                   trabajar en #ryanair como #tma:            neutral  \\\n",
       "1  @iberia @fionaferrer cuando gusten en cancún s...           neutral   \n",
       "2  sabiais que @iberia te trata muy bien en santi...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair.\\n...          negative   \n",
       "4  @cris_tortu @dakar @iberia @mitsubishi_es @bfg...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Referencia: https://stackoverflow.com/questions/6718633/python-regular-expression-again-match-url\n",
    "# remove the urls from the text but keep all the text after the url\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: re.split('http[s]*\\S+', str(x))[0])\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en ryanair como tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer cuando gusten en cancún se ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que iberia te trata muy bien en santia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair\\nb...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                      trabajar en ryanair como tma            neutral  \\\n",
       "1  iberia fionaferrer cuando gusten en cancún se ...           neutral   \n",
       "2  sabiais que iberia te trata muy bien en santia...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair\\nb...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the punctuation from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en ryanair como tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer cuando gusten en cancún se ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que iberia te trata muy bien en santia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair bu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                      trabajar en ryanair como tma            neutral  \\\n",
       "1  iberia fionaferrer cuando gusten en cancún se ...           neutral   \n",
       "2  sabiais que iberia te trata muy bien en santia...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair bu...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the \\n to a space\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the stopwrods from the text\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the emojis from the text\n",
    "\n",
    "# regular expression pattern to remove emojis from text\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # faces\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # simbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# use lambda function to remove the emojis from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the numbers from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raula\\Documents\\RAUL_ANGEL\\UVG_COMPU\\NOVENO_SEMESTRE\\DISENO-E-INOVACION\\analisis_sentimientos_depresion\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "RANDOM_SEED = 42\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "# DATASET_PATH = '/content/drive/My Drive/videos/2020-07-20/BERT_sentiment_IMDB_Dataset.csv'\n",
    "NCLASSES = 3\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cargar dataset\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# df_bert = pd.read_csv(DATASET_PATH)\n",
    "# df_bert = df[0:10000]\n",
    "\n",
    "df_bert = df_sentiment[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  airline_sentiment  is_reply  reply_count  retweet_count   \n",
      "0           neutral     False            0              0  \\\n",
      "1           neutral      True            0              0   \n",
      "2          negative     False            0              0   \n",
      "3          negative     False            0              0   \n",
      "4          positive      True            0              0   \n",
      "\n",
      "                                                text tweet_coord   \n",
      "0  Trabajar en #Ryanair como #TMA: https://t.co/r...         NaN  \\\n",
      "1  @Iberia @FIONAFERRER Cuando gusten en Cancún s...         NaN   \n",
      "2  Sabiais que @Iberia te trata muy bien en santi...         NaN   \n",
      "3  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...         NaN   \n",
      "4  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...         NaN   \n",
      "\n",
      "                    tweet_created            tweet_id tweet_location   \n",
      "0  Fri Nov 03 12:05:12 +0000 2017  926419989107798016            NaN  \\\n",
      "1  Sun Nov 26 18:40:28 +0000 2017  934854385577943041            NaN   \n",
      "2  Mon Dec 25 15:40:45 +0000 2017  945318406441635840            NaN   \n",
      "3  Mon Nov 06 14:18:35 +0000 2017  927540721296568320            NaN   \n",
      "4  Mon Jan 01 23:00:57 +0000 2018  947965901332197376            NaN   \n",
      "\n",
      "                user_timezone  \n",
      "0                      Madrid  \n",
      "1                 Mexico City  \n",
      "2                      Madrid  \n",
      "3  Pacific Time (US & Canada)  \n",
      "4                Buenos Aires  \n",
      "(7867, 10)\n",
      "@robinfood @Iberia Pues a mi les reclame un daño en mi equipaje y no\n",
      "me hicieron ni caso por no rellenar un formula…\n",
      "https://t.co/ue2dyeqxXZ\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(\"\\n\".join(wrap(df['text'][200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                               trabajar ryanair tma      0\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...      0\n",
       "2  sabiais iberia trata bien santiago chilete cam...     -1\n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...     -1\n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reajustar dataset\n",
    "df_bert['label'] = df_bert['airline_sentiment'].apply(lambda x: 1 if x == 'positive' else 0 if x == 'neutral' else -1)\n",
    "df_bert = df_bert.drop(['airline_sentiment'], axis=1)\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZACIÓN\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase:  Me gusto mucho esa pelicula!\n",
      "Tokens:  ['Me', 'gusto', 'mucho', 'esa', 'peli', '##cula', '!']\n",
      "Tokens numéricos:  [11589, 67782, 23827, 16186, 100475, 32107, 106]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo tokenización\n",
    "sample_txt = 'Me gusto mucho esa pelicula!'\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('Frase: ', sample_txt)\n",
    "print('Tokens: ', tokens)\n",
    "print('Tokens numéricos: ', token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raula\\Documents\\RAUL_ANGEL\\UVG_COMPU\\NOVENO_SEMESTRE\\DISENO-E-INOVACION\\analisis_sentimientos_depresion\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Codificación para introducir a BERT\n",
    "encoding = tokenizer.encode_plus(\n",
    "    sample_txt,\n",
    "    max_length = 10,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_token_type_ids = False,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Me', 'gusto', 'mucho', 'esa', 'peli', '##cula', '!', '[SEP]', '[PAD]']\n",
      "tensor([   101,  11589,  67782,  23827,  16186, 100475,  32107,    106,    102,\n",
      "             0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n",
    "print(encoding['input_ids'][0])\n",
    "print(encoding['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACIÓN DATASET\n",
    "\n",
    "class TEXTDataset(Dataset):\n",
    "\n",
    "  def __init__(self,texts,labels,tokenizer,max_len):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.texts)\n",
    "    \n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.texts[item])\n",
    "    label = self.labels[item]\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length = self.max_len,\n",
    "        truncation = True,\n",
    "        add_special_tokens = True,\n",
    "        return_token_type_ids = False,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "        )\n",
    "    \n",
    "\n",
    "    return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'label': torch.tensor(label, dtype=torch.long)\n",
    "      } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader:\n",
    "\n",
    "def data_loader(df, tokenizer, max_len, batch_size):\n",
    "  dataset = TEXTDataset(\n",
    "      texts = df.text.to_numpy(),\n",
    "      labels = df.label.to_numpy(),\n",
    "      tokenizer = tokenizer,\n",
    "      max_len = MAX_LEN\n",
    "  )\n",
    "\n",
    "  return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bert, df_test_bert = train_test_split(df_bert, test_size = 0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_data_loader = data_loader(df_train_bert, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = data_loader(df_test_bert, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Model\n",
    "class BERTSentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(BERTSentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, cls_output = self.bert(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask\n",
    "    )\n",
    "    drop_output = self.drop(cls_output)\n",
    "    return self.linear(drop_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERTSentimentClassifier(NCLASSES)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raula\\Documents\\RAUL_ANGEL\\UVG_COMPU\\NOVENO_SEMESTRE\\DISENO-E-INOVACION\\analisis_sentimientos_depresion\\venv\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteración entrenamiento\n",
    "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    correct_predictions += torch.sum(preds == labels)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double()/n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['label'].to(device)\n",
    "      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      correct_predictions += torch.sum(preds == labels)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 de 5\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento!!!\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1} de {EPOCHS}')\n",
    "  print('------------------')\n",
    "  train_acc, train_loss = train_model(\n",
    "      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train_bert)\n",
    "  )\n",
    "  test_acc, test_loss = eval_model(\n",
    "      model, test_data_loader, loss_fn, device, len(df_test_bert)\n",
    "  )\n",
    "  print(f'Entrenamiento: Loss: {train_loss}, accuracy: {train_acc}')\n",
    "  print(f'Validación: Loss: {test_loss}, accuracy: {test_acc}')\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySentiment(text):\n",
    "  encoding_review = tokenizer.encode_plus(\n",
    "      text,\n",
    "      max_length = MAX_LEN,\n",
    "      truncation = True,\n",
    "      add_special_tokens = True,\n",
    "      return_token_type_ids = False,\n",
    "      pad_to_max_length = True,\n",
    "      return_attention_mask = True,\n",
    "      return_tensors = 'pt'\n",
    "      )\n",
    "  \n",
    "  input_ids = encoding_review['input_ids'].to(device)\n",
    "  attention_mask = encoding_review['attention_mask'].to(device)\n",
    "  output = model(input_ids, attention_mask)\n",
    "  _, prediction = torch.max(output, dim=1)\n",
    "  print('predicción = ', prediction)\n",
    "  print(\"\\n\".join(wrap(text)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
