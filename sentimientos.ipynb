{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo para anlisis de sentmientos para luego precedir si tiene sintomas de depresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Nov 03 12:05:12 +0000 2017</td>\n",
       "      <td>926419989107798016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Nov 26 18:40:28 +0000 2017</td>\n",
       "      <td>934854385577943041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 25 15:40:45 +0000 2017</td>\n",
       "      <td>945318406441635840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Nov 06 14:18:35 +0000 2017</td>\n",
       "      <td>927540721296568320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 01 23:00:57 +0000 2018</td>\n",
       "      <td>947965901332197376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  is_reply  reply_count  retweet_count   \n",
       "0           neutral     False            0              0  \\\n",
       "1           neutral      True            0              0   \n",
       "2          negative     False            0              0   \n",
       "3          negative     False            0              0   \n",
       "4          positive      True            0              0   \n",
       "\n",
       "                                                text tweet_coord   \n",
       "0  Trabajar en #Ryanair como #TMA: https://t.co/r...         NaN  \\\n",
       "1  @Iberia @FIONAFERRER Cuando gusten en Cancún s...         NaN   \n",
       "2  Sabiais que @Iberia te trata muy bien en santi...         NaN   \n",
       "3  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...         NaN   \n",
       "4  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...         NaN   \n",
       "\n",
       "                    tweet_created            tweet_id tweet_location   \n",
       "0  Fri Nov 03 12:05:12 +0000 2017  926419989107798016            NaN  \\\n",
       "1  Sun Nov 26 18:40:28 +0000 2017  934854385577943041            NaN   \n",
       "2  Mon Dec 25 15:40:45 +0000 2017  945318406441635840            NaN   \n",
       "3  Mon Nov 06 14:18:35 +0000 2017  927540721296568320            NaN   \n",
       "4  Mon Jan 01 23:00:57 +0000 2018  947965901332197376            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                      Madrid  \n",
       "1                 Mexico City  \n",
       "2                      Madrid  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                Buenos Aires  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data from tweets_public.csv and create a dataframe\n",
    "df = pd.read_csv('data/tweets_public.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with only the text and airline_sentiment columns and tweet id with the name df_sentiment\n",
    "df_sentiment = df[['text', 'airline_sentiment', 'tweet_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Temp\\ipykernel_7712\\3390815067.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sentiment['text'] = df_sentiment['text'].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en #ryanair como #tma: https://t.co/r...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancún s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair.\\n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @iberia @mitsubishi_es @bfg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0  trabajar en #ryanair como #tma: https://t.co/r...           neutral  \\\n",
       "1  @iberia @fionaferrer cuando gusten en cancún s...           neutral   \n",
       "2  sabiais que @iberia te trata muy bien en santi...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair.\\n...          negative   \n",
       "4  @cris_tortu @dakar @iberia @mitsubishi_es @bfg...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the text letters to lowercase\n",
    "df_sentiment['text'] = df_sentiment['text'].str.lower()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en #ryanair como #tma:</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancún s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair.\\n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @iberia @mitsubishi_es @bfg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                   trabajar en #ryanair como #tma:            neutral  \\\n",
       "1  @iberia @fionaferrer cuando gusten en cancún s...           neutral   \n",
       "2  sabiais que @iberia te trata muy bien en santi...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair.\\n...          negative   \n",
       "4  @cris_tortu @dakar @iberia @mitsubishi_es @bfg...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Referencia: https://stackoverflow.com/questions/6718633/python-regular-expression-again-match-url\n",
    "# remove the urls from the text but keep all the text after the url\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: re.split('http[s]*\\S+', str(x))[0])\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en ryanair como tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer cuando gusten en cancún se ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que iberia te trata muy bien en santia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair\\nb...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                      trabajar en ryanair como tma            neutral  \\\n",
       "1  iberia fionaferrer cuando gusten en cancún se ...           neutral   \n",
       "2  sabiais que iberia te trata muy bien en santia...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair\\nb...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the punctuation from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar en ryanair como tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer cuando gusten en cancún se ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais que iberia te trata muy bien en santia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis el café de ryanair bu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                      trabajar en ryanair como tma            neutral  \\\n",
       "1  iberia fionaferrer cuando gusten en cancún se ...           neutral   \n",
       "2  sabiais que iberia te trata muy bien en santia...          negative   \n",
       "3  nunca nunca nunca pidáis el café de ryanair bu...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the \\n to a space\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the stopwrods from the text\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the emojis from the text\n",
    "\n",
    "# regular expression pattern to remove emojis from text\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # faces\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # simbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# use lambda function to remove the emojis from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trabajar ryanair tma</td>\n",
       "      <td>neutral</td>\n",
       "      <td>926419989107798016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iberia fionaferrer gusten cancún viaja disfrut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>934854385577943041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabiais iberia trata bien santiago chilete cam...</td>\n",
       "      <td>negative</td>\n",
       "      <td>945318406441635840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nunca nunca nunca pidáis café ryanair bueno ve...</td>\n",
       "      <td>negative</td>\n",
       "      <td>927540721296568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cristortu dakar iberia mitsubishies bfgoodrich...</td>\n",
       "      <td>positive</td>\n",
       "      <td>947965901332197376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   \n",
       "0                               trabajar ryanair tma           neutral  \\\n",
       "1  iberia fionaferrer gusten cancún viaja disfrut...           neutral   \n",
       "2  sabiais iberia trata bien santiago chilete cam...          negative   \n",
       "3  nunca nunca nunca pidáis café ryanair bueno ve...          negative   \n",
       "4  cristortu dakar iberia mitsubishies bfgoodrich...          positive   \n",
       "\n",
       "             tweet_id  \n",
       "0  926419989107798016  \n",
       "1  934854385577943041  \n",
       "2  945318406441635840  \n",
       "3  927540721296568320  \n",
       "4  947965901332197376  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the numbers from the text\n",
    "df_sentiment.loc[:, 'text'] = df_sentiment['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Temp\\ipykernel_7712\\4261467530.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sentiment['label'] = df_sentiment['airline_sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7862</th>\n",
       "      <td>@Iberia @iberiaexpress especialistas en dejart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7863</th>\n",
       "      <td>Con @Iberia, mi destino a un solo click. ¡Dese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>@Iberia Muy bien. Muchas gracias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>Es que volar con Ryanair es peor que irte a ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>Iberia inaugura un nuevo espacio Premium para ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7867 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Trabajar en #Ryanair como #TMA: https://t.co/r...      1\n",
       "1     @Iberia @FIONAFERRER Cuando gusten en Cancún s...      1\n",
       "2     Sabiais que @Iberia te trata muy bien en santi...      0\n",
       "3     NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...      0\n",
       "4     @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...      1\n",
       "...                                                 ...    ...\n",
       "7862  @Iberia @iberiaexpress especialistas en dejart...      0\n",
       "7863  Con @Iberia, mi destino a un solo click. ¡Dese...      1\n",
       "7864                   @Iberia Muy bien. Muchas gracias      1\n",
       "7865  Es que volar con Ryanair es peor que irte a ch...      0\n",
       "7866  Iberia inaugura un nuevo espacio Premium para ...      0\n",
       "\n",
       "[7867 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = df[['text', 'airline_sentiment']]\n",
    "# df_sentiment['label'] = df_sentiment['airline_sentiment'].apply(lambda x: '1 start' if x == 'negative' else '5 stars')\n",
    "df_sentiment['label'] = df_sentiment['airline_sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df_sentiment = df_sentiment[['text', 'label']]\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df_sentiment.to_csv('data/clean_tweets_sentiment.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preload1 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# model_preload = \"distilbert-base-uncased\"\n",
    "# model_preload = \"xlm-roberta-base\"\n",
    "# model_preload = \"distilbert-base-multilingual-cased\"\n",
    "# model_preload = \"bert-base-multilingual-cased\"\n",
    "model_preload =  'dccuchile/bert-base-spanish-wwm-cased'\n",
    "# model_preload = \"RoBERTa-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\"Amo el mundo en el que vivo\", \"Odio a todas las personas que conozco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9905071258544922},\n",
       " {'label': 'NEGATIVE', 'score': 0.9892126321792603}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9994358420372009}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline('no estoy normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7264281511306763},\n",
       " {'label': '5 stars', 'score': 0.6389875411987305}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model = pipeline(\"sentiment-analysis\", model=model_preload1)\n",
    "specific_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '3 stars', 'score': 0.3511374890804291}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model('estoy normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.5364329814910889}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model('estoy triste')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/raula/.cache/huggingface/datasets/csv/default-166099f8577deca8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7239aed7bba4b6dbb42b1eb3ccae184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b47e9906dfa401bb94f7a388e9a401c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9158e7c8a8c74c658e63186372af9838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/raula/.cache/huggingface/datasets/csv/default-166099f8577deca8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/raula/.cache/huggingface/datasets/csv/default-166099f8577deca8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "local_train = load_dataset(\"csv\", data_files=\"data/clean_tweets_sentiment.csv\", split=\"train[:70%]\")\n",
    "local_test = load_dataset(\"csv\", data_files=\"data/clean_tweets_sentiment.csv\", split=\"train[70%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5507, 2)\n",
      "(2360, 2)\n",
      "{'text': 'Trabajar en #Ryanair como #TMA: https://t.co/ruUArBe1tO #empleo', 'label': 1}\n",
      "{'text': '@radioledonline @Iberia @lauraluzo Sr. Gallego, por qué no pagáis las indemnizaciones por cancelaciones? Aviación C… https://t.co/pC3cvISmjH', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(local_train.shape)\n",
    "print(local_test.shape)\n",
    "print(local_train[0])\n",
    "print(local_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = local_train.shuffle(seed=42).select(\n",
    "    list(list(range(300)))\n",
    ")\n",
    "small_test_dataset = local_test.shuffle(seed=42).select(list(list(range(100))))\n",
    "# # split df_sentiment in 70% train and 30% test\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, test = train_test_split(df_sentiment, test_size=0.3, random_state=42)\n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_preload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6154e85e2dc642ebba5d67017ab1d013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41b1a8abc94fad88f4bbe7b81d7800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# user the function preprocess_function to tokenize the train and test data\n",
    "# tokenized_train = train['text'].apply(lambda x: tokenizer(x, truncation=True))\n",
    "# tokenized_test = test['text'].apply(lambda x: tokenizer(x, truncation=True))\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_preload, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1739dbdec954418580bc3cd6faed351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raula\\Documents\\RAUL_ANGEL\\UVG_COMPU\\NOVENO_SEMESTRE\\DISENO-E-INOVACION\\analisis_sentimientos_depresion\\raulangelj/huggingface_sentiment_analysis is already a clone of https://huggingface.co/raulangelj/huggingface_sentiment_analysis. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"raulangelj/huggingface_sentiment_analysis\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=25,\n",
    "   per_device_eval_batch_size=25,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=True,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\raula/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\raula\\Documents\\RAUL_ANGEL\\UVG_COMPU\\NOVENO_SEMESTRE\\DISENO-E-INOVACION\\analisis_sentimientos_depresion\\wandb\\run-20230526_214919-ubzee56h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raulangel/huggingface/runs/ubzee56h' target=\"_blank\">sparkling-water-1</a></strong> to <a href='https://wandb.ai/raulangel/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raulangel/huggingface' target=\"_blank\">https://wandb.ai/raulangel/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raulangel/huggingface/runs/ubzee56h' target=\"_blank\">https://wandb.ai/raulangel/huggingface/runs/ubzee56h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08b6d9893fe43c098e5940ccccdb965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 628.1913, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.038, 'train_loss': 0.6502002477645874, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.6502002477645874, metrics={'train_runtime': 628.1913, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.038, 'train_loss': 0.6502002477645874, 'epoch': 2.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216ff3ee78b64916aa43157febf0e9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\AppData\\Local\\Temp\\ipykernel_7712\\1757224399.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6655668020248413,\n",
       " 'eval_accuracy': 0.59,\n",
       " 'eval_f1': 0.6666666666666667,\n",
       " 'eval_runtime': 2.4202,\n",
       " 'eval_samples_per_second': 41.319,\n",
       " 'eval_steps_per_second': 1.653,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "To https://huggingface.co/raulangelj/huggingface_sentiment_analysis\n",
      "   bad9ad3..998b40d  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amo el mundo en el que vivo', 'Odio a todas las personas que conozco']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.567038893699646},\n",
       " {'label': 'LABEL_1', 'score': 0.508574903011322}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "sentiment_model = pipeline(model=repo_name)\n",
    "sentiment_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5060250759124756}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model('no me siento bien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5540620684623718}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model('hoy es el mejor dia de mi vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5562945008277893}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model('ya no puedo con mi vida, no quiero que me hablen')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización\n",
    "RANDOM_SEED = 42\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "# DATASET_PATH = '/content/drive/My Drive/videos/2020-07-20/BERT_sentiment_IMDB_Dataset.csv'\n",
    "NCLASSES = 3\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cargar dataset\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# df_bert = pd.read_csv(DATASET_PATH)\n",
    "# df_bert = df[0:10000]\n",
    "\n",
    "df_bert = df_sentiment[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(\"\\n\".join(wrap(df['text'][200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reajustar dataset\n",
    "df_bert['label'] = df_bert['airline_sentiment'].apply(lambda x: 1 if x == 'positive' else 0 if x == 'neutral' else -1)\n",
    "df_bert = df_bert.drop(['airline_sentiment'], axis=1)\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZACIÓN\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo tokenización\n",
    "sample_txt = 'Me gusto mucho esa pelicula!'\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('Frase: ', sample_txt)\n",
    "print('Tokens: ', tokens)\n",
    "print('Tokens numéricos: ', token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación para introducir a BERT\n",
    "encoding = tokenizer.encode_plus(\n",
    "    sample_txt,\n",
    "    max_length = 10,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_token_type_ids = False,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n",
    "print(encoding['input_ids'][0])\n",
    "print(encoding['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACIÓN DATASET\n",
    "\n",
    "class TEXTDataset(Dataset):\n",
    "\n",
    "  def __init__(self,texts,labels,tokenizer,max_len):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.texts)\n",
    "    \n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.texts[item])\n",
    "    label = self.labels[item]\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length = self.max_len,\n",
    "        truncation = True,\n",
    "        add_special_tokens = True,\n",
    "        return_token_type_ids = False,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "        )\n",
    "    \n",
    "\n",
    "    return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'label': torch.tensor(label, dtype=torch.long)\n",
    "      } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader:\n",
    "\n",
    "def data_loader(df, tokenizer, max_len, batch_size):\n",
    "  dataset = TEXTDataset(\n",
    "      texts = df.text.to_numpy(),\n",
    "      labels = df.label.to_numpy(),\n",
    "      tokenizer = tokenizer,\n",
    "      max_len = MAX_LEN\n",
    "  )\n",
    "\n",
    "  return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bert, df_test_bert = train_test_split(df_bert, test_size = 0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_data_loader = data_loader(df_train_bert, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = data_loader(df_test_bert, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Model\n",
    "class BERTSentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(BERTSentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, cls_output = self.bert(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask\n",
    "    )\n",
    "    drop_output = self.drop(cls_output)\n",
    "    return self.linear(drop_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTSentimentClassifier(NCLASSES)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteración entrenamiento\n",
    "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    correct_predictions += torch.sum(preds == labels)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double()/n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['label'].to(device)\n",
    "      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      correct_predictions += torch.sum(preds == labels)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento!!!\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1} de {EPOCHS}')\n",
    "  print('------------------')\n",
    "  train_acc, train_loss = train_model(\n",
    "      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train_bert)\n",
    "  )\n",
    "  test_acc, test_loss = eval_model(\n",
    "      model, test_data_loader, loss_fn, device, len(df_test_bert)\n",
    "  )\n",
    "  print(f'Entrenamiento: Loss: {train_loss}, accuracy: {train_acc}')\n",
    "  print(f'Validación: Loss: {test_loss}, accuracy: {test_acc}')\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySentiment(text):\n",
    "  encoding_review = tokenizer.encode_plus(\n",
    "      text,\n",
    "      max_length = MAX_LEN,\n",
    "      truncation = True,\n",
    "      add_special_tokens = True,\n",
    "      return_token_type_ids = False,\n",
    "      pad_to_max_length = True,\n",
    "      return_attention_mask = True,\n",
    "      return_tensors = 'pt'\n",
    "      )\n",
    "  \n",
    "  input_ids = encoding_review['input_ids'].to(device)\n",
    "  attention_mask = encoding_review['attention_mask'].to(device)\n",
    "  output = model(input_ids, attention_mask)\n",
    "  _, prediction = torch.max(output, dim=1)\n",
    "  print('predicción = ', prediction)\n",
    "  print(\"\\n\".join(wrap(text)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
